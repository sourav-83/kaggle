{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"hey there! let's get started ...\nlet's import pandas & numpy","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:16:31.463664Z","iopub.execute_input":"2024-11-30T03:16:31.464045Z","iopub.status.idle":"2024-11-30T03:16:31.468836Z","shell.execute_reply.started":"2024-11-30T03:16:31.464011Z","shell.execute_reply":"2024-11-30T03:16:31.467701Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"let's import our train & test datasets","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:16:36.337523Z","iopub.execute_input":"2024-11-30T03:16:36.338046Z","iopub.status.idle":"2024-11-30T03:16:36.418554Z","shell.execute_reply.started":"2024-11-30T03:16:36.337995Z","shell.execute_reply":"2024-11-30T03:16:36.417408Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"let's see how the dataset looks","metadata":{}},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:16:42.653057Z","iopub.execute_input":"2024-11-30T03:16:42.653445Z","iopub.status.idle":"2024-11-30T03:16:42.683286Z","shell.execute_reply.started":"2024-11-30T03:16:42.653405Z","shell.execute_reply":"2024-11-30T03:16:42.682092Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"         id keyword location  \\\n0         1     NaN      NaN   \n1         4     NaN      NaN   \n2         5     NaN      NaN   \n3         6     NaN      NaN   \n4         7     NaN      NaN   \n...     ...     ...      ...   \n7608  10869     NaN      NaN   \n7609  10870     NaN      NaN   \n7610  10871     NaN      NaN   \n7611  10872     NaN      NaN   \n7612  10873     NaN      NaN   \n\n                                                   text  target  \n0     Our Deeds are the Reason of this #earthquake M...       1  \n1                Forest fire near La Ronge Sask. Canada       1  \n2     All residents asked to 'shelter in place' are ...       1  \n3     13,000 people receive #wildfires evacuation or...       1  \n4     Just got sent this photo from Ruby #Alaska as ...       1  \n...                                                 ...     ...  \n7608  Two giant cranes holding a bridge collapse int...       1  \n7609  @aria_ahrary @TheTawniest The out of control w...       1  \n7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n7611  Police investigating after an e-bike collided ...       1  \n7612  The Latest: More Homes Razed by Northern Calif...       1  \n\n[7613 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>10869</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>10870</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7610</th>\n      <td>10871</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7611</th>\n      <td>10872</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Police investigating after an e-bike collided ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>10873</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"The 'target' column indicates whether a tweet is disaster-related (1) or not (0). By using the .value_counts() method, we can see the distribution of these classes in the training dataset.","metadata":{}},{"cell_type":"code","source":"df['target'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:16:55.897120Z","iopub.execute_input":"2024-11-30T03:16:55.897523Z","iopub.status.idle":"2024-11-30T03:16:55.905637Z","shell.execute_reply.started":"2024-11-30T03:16:55.897486Z","shell.execute_reply":"2024-11-30T03:16:55.904478Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"target\n0    4342\n1    3271\nName: count, dtype: int64"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"Before analyzing text data, we need to preprocess it. This function, clean_text, removes or replaces unwanted characters and punctuation in the text.","metadata":{}},{"cell_type":"code","source":"def clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:17:05.999522Z","iopub.execute_input":"2024-11-30T03:17:05.999962Z","iopub.status.idle":"2024-11-30T03:17:06.006393Z","shell.execute_reply.started":"2024-11-30T03:17:05.999900Z","shell.execute_reply":"2024-11-30T03:17:06.005182Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"let's apply this function on our datasets now","metadata":{}},{"cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(lambda x: clean_text(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:17:19.171957Z","iopub.execute_input":"2024-11-30T03:17:19.172361Z","iopub.status.idle":"2024-11-30T03:17:19.227376Z","shell.execute_reply.started":"2024-11-30T03:17:19.172307Z","shell.execute_reply":"2024-11-30T03:17:19.226163Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"test[\"text\"] = test[\"text\"].apply(lambda x: clean_text(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:18:19.871978Z","iopub.execute_input":"2024-11-30T03:18:19.872390Z","iopub.status.idle":"2024-11-30T03:18:19.896478Z","shell.execute_reply.started":"2024-11-30T03:18:19.872353Z","shell.execute_reply":"2024-11-30T03:18:19.895241Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"let's split our first dataset into train and evaluation parts ","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n\n\ntrain_texts, train_labels = train_df['text'].tolist(), train_df['target'].tolist()\ntest_texts, test_labels = test_df['text'].tolist(), test_df['target'].tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:19:00.282550Z","iopub.execute_input":"2024-11-30T03:19:00.282927Z","iopub.status.idle":"2024-11-30T03:19:01.040066Z","shell.execute_reply.started":"2024-11-30T03:19:00.282891Z","shell.execute_reply":"2024-11-30T03:19:01.038825Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"let's import the tokenizer for our model 'distilbert-base-uncased' from hugging-face transformers and tokenize our train and evaluation texts","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use PyTorch to handle our data for deep learning. In this step, we convert the train_labels and test_labels into PyTorch tensors using the torch.tensor() function.","metadata":{}},{"cell_type":"code","source":"import torch\n\ntrain_labels = torch.tensor(train_labels)\ntest_labels = torch.tensor(test_labels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To train a PyTorch model, we need to organize our data using the Dataset class. Here, we define a custom class, DisasterTweetsDataset, which inherits from torch.utils.data.Dataset. This custom dataset simplifies managing and accessing the encoded tweets and their labels.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass DisasterTweetsDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n\ntrain_dataset = DisasterTweetsDataset(train_encodings, train_labels)\ntest_dataset = DisasterTweetsDataset(test_encodings, test_labels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use the TrainingArguments class from the 🤗 Transformers library to configure our training process. This class allows us to define various parameters for training and evaluation.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',        \n    evaluation_strategy=\"epoch\",  \n    save_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    logging_dir='./logs',\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    save_total_limit=1, \n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To evaluate the performance of our model, we define a custom function, compute_metrics, which calculates the F1-score using the predictions and true labels.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    f1 = f1_score(labels, predictions, average=\"weighted\")\n    return {\"f1\": f1}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To train and evaluate our model, we use the AutoModelForSequenceClassification class and the Trainer from the 🤗 Transformers library.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, Trainer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", \n    num_labels=2  \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The code snippet disables the Weights & Biases (W&B) integration by setting the environment variable WANDB_DISABLED to \"true\". W&B is a popular tool for tracking experiments and visualizing model performance, but in some cases, you may want to disable it.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"let's trainnn ...","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this step, we use the tokenizer to preprocess the text data in the test set. The tokenizer converts the raw text into a format that can be fed into the model for prediction.","metadata":{}},{"cell_type":"code","source":"\ntest_encodings = tokenizer(\n    test['text'].tolist(),  \n    truncation=True,\n    padding=True,\n    max_length=128,\n    return_tensors=\"pt\"  \n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After preparing the test data, we use the trained model to make predictions. The process is done in inference mode (without updating model weights) using torch.no_grad().","metadata":{}},{"cell_type":"code","source":"\nwith torch.no_grad(): \n    outputs = model(**test_encodings)  \n    logits = outputs.logits \n    predictions = torch.argmax(logits, dim=1) \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"it's time to create the submission dataframe","metadata":{}},{"cell_type":"code","source":"\nsubmission = pd.DataFrame({\n    'id': test['id'],          \n    'target': predictions.numpy()  \n})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally, let's save our submission file in csv format ...","metadata":{}},{"cell_type":"code","source":"\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Feel free to play with the training arguments & leave an upvote if you found this notebook helpful ...","metadata":{}}]}